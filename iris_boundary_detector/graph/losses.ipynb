{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from scipy.ndimage import distance_transform_edt as distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, base_lr, decay_rate, step_size, epoch):\n",
    "    \"\"\"Set the learning rate to the initial LR decayed by decay_rate(ExpLR)\"\"\"\n",
    "    lr = base_lr * decay_rate**(epoch//step_size)\n",
    "    lr = max(lr, 0.0005)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference from https://github.com/AayushKrChaudhary/RITnet/blob/master/utils.py\n",
    "class GeneralizedDiceLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5, weight=None, softmax=True, reduction=True):\n",
    "        super(GeneralizedDiceLoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.weight = []\n",
    "        self.reduction = reduction\n",
    "        if softmax:\n",
    "            self.norm = nn.Softmax(dim=1)\n",
    "        else:\n",
    "            self.norm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, ip, target):\n",
    "        Label = (np.arange(3) == target.cpu().numpy()[..., None]).astype(np.uint8)\n",
    "        target = torch.from_numpy(np.rollaxis(Label, 3,start=1)).cuda()\n",
    "\n",
    "        assert ip.shape == target.shape\n",
    "        ip = self.norm(ip)\n",
    "        ip = torch.flatten(ip, start_dim=2, end_dim=-1).cuda().to(torch.float32) \n",
    "        target = torch.flatten(target, start_dim=2, end_dim=-1).cuda().to(torch.float32) \n",
    "        \n",
    "        numerator = ip*target \n",
    "        denominator = ip + target \n",
    "\n",
    "        class_weights = 1./(torch.sum(target, dim=2)**2).clamp(min=self.epsilon)\n",
    "\n",
    "        A = class_weights*torch.sum(numerator, dim=2)\n",
    "        B = class_weights*torch.sum(denominator, dim=2) \n",
    "\n",
    "        dice_metric = 2.*torch.sum(A, dim=1)/torch.sum(B, dim=1)\n",
    "        \n",
    "        if self.reduction:\n",
    "            return torch.mean(1. - dice_metric.clamp(min=self.epsilon))\n",
    "        else:\n",
    "            return 1. - dice_metric.clamp(min=self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance map loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference from https://github.com/LIVIAETS/surface-loss\n",
    "def one_hot2dist(posmask):\n",
    "    assert len(posmask.shape) == 2\n",
    "    h, w = posmask.shape \n",
    "    res = np.zeros_like(posmask)\n",
    "    posmask = posmask.astype(np.bool)\n",
    "    mxDist = np.sqrt((h-1)**2 + (w-1)**2)\n",
    "    if posmask.any():\n",
    "        negmask = ~posmask \n",
    "        # distance(): Calculate the distance from the non-zero pixel to the nearest zero pixel in the image\n",
    "        res = distance(negmask) * negmask - (distance(posmask) - 1) * posmask \n",
    "    return res/mxDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceMapLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-5, softmax=True):\n",
    "        super(DistanceMapLoss, self).__init__()\n",
    "        self.weight_map = []\n",
    "    def forward(self, x, distmap):\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        self.weight_map = distmap \n",
    "        score = x.flatten(start_dim=2)*distmap.flatten(start_dim=2)\n",
    "        score = torch.mean(score, dim=2) # Mean between pixels per channel \n",
    "        score = torch.mean(score, dim=1) # Mean between channels \n",
    "        score = torch.mean(score)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, size_average=True):\n",
    "        super(MissingLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.contiguous().view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.contiguous().view(-1,1) # N,H,W => N*H*W,1\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=-1) # N*H*W,C\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1) # N*H*W\n",
    "        pt = Variable(logpt.data.exp()) \n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        # Full loss\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
